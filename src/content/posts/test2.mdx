---
title: 'testing with a blog'
description: 'ai cost between high and not high'
---


Alright techies, buckle up! I just watched a video that completely flipped my understanding of AI costs, and I had to share the breakdown with you. Turns out, the "AI is getting cheaper" narrative I, and many others, bought into is only half the story. Things are getting expensive FAST.

The video, titled "I was wrong about AI costs (they keep going up)," comes from a creator who isn't afraid to dive deep and get their hands dirty (and their wallet significantly lighter) by running real-world benchmarks. They openly admit to being "misled" by those seemingly promising charts showing plummeting token costs. But what's *really* going on? Let's break it down.

## The Token Cost Illusion: Why Those Charts Don't Tell the Whole Story

We've all seen the graphs â€“ AI model costs per million tokens are dropping like crazy! Makes you think you can just throw anything at these models and build amazing things on a shoestring budget, right? Wrong. Dead wrong.

The video highlights the crucial distinction between *cost per token* and the *actual cost of using the models.* The creator used Grok 4 as a prime example. On paper, Grok 4 looks relatively affordable. But when they ran SkateBench, a benchmark they developed to test an AI's ability to name skateboarding tricks, Grok 4 ended up costing 2.5 times more than Gemini 2.5 Pro and a *whopping* 20-30 times more than Claude 4 Opus, even though Opus has a way higher cost per token according to paper.

### Reasoning: The Hidden Cost Driver

The key culprit? Reasoning. Newer AI models, especially "reasoning models," are generating *way* more output tokens than their predecessors, and that output comes at a premium. Input is fairly cheap, the output kills your wallet.

Think of it this way: you ask a non-reasoning model a question, and it gives you a direct answer.  Now, you ask a reasoning model the same question. It spends time "thinking," generating internal "reasoning" tokens, and *then* gives you the answer. The creator's skateboard trick example perfectly illustrates this. A non-reasoning model generated seven output tokens, while Grok 4 spewed out a ridiculous 603, for the exact same request!

### The Problem With "Unlimited" and the Token Short Squeeze

This excessive token generation creates a "token short squeeze" that's making life miserable for AI companies, especially those offering "unlimited" or fixed-price subscriptions. The video dives into an article by Ethan Mollick that perfectly captures this problem.

Here's the gist:

*   **The Myth of Dropping Costs:** While the cost of older models is decreasing, everyone wants to use the *best* model, and those always cost about the same at the cutting edge.
*   **The Agent Problem:** AI agents (systems that can autonomously perform tasks) are now consuming massive amounts of tokens. A task that used to take 1,000 tokens now takes 100,000. Think of a deep research run for 20 minutes, that could cost about a dollar today, and by 2027 could cost $72 per day per user. And once people discover the "for loop", you are bankrupt.
*   **The "Unlimited" Trap:** Offering "unlimited" usage is a race to the bottom. Users will find ways to max out the system, leading to unsustainable costs. Anthropic tried to cover this short squeeze, offering a max subscription, but still got obliterated.

This leads to a prisoner's dilemma. Everyone knows usage-based pricing would be more sustainable, but nobody wants to be the first to switch because users will flock to the "unlimited" competitor.

## What Can We Do? Actionable Advice for Navigating the Expensive AI Landscape

So, how do we navigate this costly new world? Here are some takeaways from the video and my own thoughts:

*   **Be Smart About Model Selection:** Don't always default to the most powerful (and expensive) model.  For many tasks, a smaller, non-reasoning model like Gemini 2.0 Flash is perfectly adequate and much cheaper.
*   **Control Reasoning:** Some platforms, like T3Chat, allow you to adjust the reasoning effort of a model. Be mindful of this setting! High reasoning can significantly increase costs without a noticeable improvement in output quality.
*   **Vertical Integration:** The creator mentions Replit's strategy of vertical integration. Bundle AI with other services (hosting, databases, etc.) and make money on those other layers of the stack. This allows you to use AI as a "loss leader" to drive consumption of those other services.
*   **Consider the Cost-Benefit:** When designing AI-powered applications, always weigh the cost of token consumption against the value you're providing to the user. Is that extra bit of "reasoning" really worth the added expense?
*   **Embrace Usage-Based Pricing (If You Can):** It's not popular, but it's the only sustainable option in the long run.

## The Bottom Line: AI Is Amazing, But It's Not Free

The video serves as a stark reminder that AI is not a magical money-saving machine. While AI models have a *ton* of potential, they come with real costs that are, in many cases, increasing due to factors like model sophistication and token consumption.

The days of throwing unlimited prompts at an AI and hoping for the best are over. We need to be more strategic and mindful of how we use these powerful tools. It's time to face the reality that AI, while potentially revolutionary, is not a free lunch. We have to be cost aware and smart with our AI usage!

Now if you'll excuse me, I have to go check my AI billing statements...I have a feeling I'm going to be sweating for a while.
